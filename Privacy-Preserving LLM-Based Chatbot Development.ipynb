{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT API: OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "API_KEY = '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'\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1+1 = 2\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model = 'text-davinci-003', temperature=0)\n",
    "prompt = \"What is 1+1?\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\"https://projectstanley.substack.com/p/generative-ai-will-boost-a-lot-more\", \\\n",
    "        \"https://projectstanley.substack.com/p/the-true-promise-and-peril-of-conversational\", \\\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\", \\\n",
    "        \"https://www.computerworld.com/article/3697649/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html\", \\\n",
    "        \"https://arxiv.org/pdf/2210.12887.pdf\",\\\n",
    "        \"https://en.wikipedia.org/wiki/Audit\"]\n",
    "\n",
    "# each doc is a website\n",
    "doc = []\n",
    "\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    doc.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'title': \"LLM Powered Autonomous Agents | Lil'Log\",\n",
       " 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting docs to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use recursiveCharacter as the splitter\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_splitted = r_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_splitted[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage - Vectorstores and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(openai_api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(docs_splitted)):\n",
    "#     docs_embeded = embedding.embed_query(docs_splitted[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# transfer function \n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import chromadb\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "def embedding(doc):\n",
    "    # embeddings function\n",
    "    embedding = OpenAIEmbeddings(openai_api_key = API_KEY)\n",
    "\n",
    "    # creat vector database and collestion\n",
    "    client = chromadb.EphemeralClient()\n",
    "    collection = client.create_collection(name='my_collection')\n",
    "\n",
    "    # create idx in collection add\n",
    "    idx = []\n",
    "    for i in range(len(doc)):\n",
    "        tem = 'id' + str(i)\n",
    "        idx.append(tem)\n",
    "\n",
    "    # add vector into the database\n",
    "    for i in range(len(idx)):\n",
    "        collection.add(\n",
    "            documents=doc[i].page_content,\n",
    "            ids=idx[i])\n",
    "        \n",
    "    return collection  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = embedding(docs_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer function\n",
    "\n",
    "def get_answer(query_text):\n",
    "    memory = []\n",
    "    \n",
    "    # retrive the vector database\n",
    "    results = collection.query(query_texts = query_text ,n_results=3)\n",
    "    first_doc = results['documents'][0][0]\n",
    "    memory.append([query_text, first_doc])\n",
    "    \n",
    "    # prompt\n",
    "    prompt = f\"\"\"\n",
    "            You are a friendly consultant. You are answering the questions to people who have no background of business and machine learning.\n",
    "            \n",
    "            Your task is to generate a short summary of the paragraph.\n",
    "\n",
    "            Summarize the paragraph below, delimited by triple \n",
    "            backticks, in at most 50 words. \n",
    "            \n",
    "            paragraph:'''{first_doc}'''\n",
    "        \n",
    "        \n",
    "            And please list two parts: User's request and your answer. Follow the format below.\n",
    "            \n",
    "            \n",
    "            format:'''\n",
    "            \n",
    "            - User_request: {query_text}\n",
    "            \n",
    "            \n",
    "            - Answer : Your answer\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "    # transfer vector to human language by llm\n",
    "    answer = llm(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.get('id2')['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is LLM?\n"
     ]
    }
   ],
   "source": [
    "user_request = input('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            - User_request: What is LLM?\\n            \\n            - Answer: LLM is a machine-learning neuro network trained through data input/output sets. It uses self-supervised or semi-supervised learning methodology to predict the next word. It requires massive server farms to train and is controlled by parameters. It can provide probable answers based on existing data.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
